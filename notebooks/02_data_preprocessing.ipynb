{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7791fa13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Setup and Load Data\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from preprocessor import DataPreprocessor\n",
    "from data_loader import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "loader = DataLoader('../data/raw/historical_data.csv', '../data/raw/fear_greed_index.csv')\n",
    "trader_data, sentiment_data = loader.load_all_data()\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "print(\"Original data shapes:\")\n",
    "print(f\"Trader data: {trader_data.shape}\")\n",
    "print(f\"Sentiment data: {sentiment_data.shape}\")\n",
    "\n",
    "# Cell 2: Preprocess Trader Data\n",
    "print(\"=== PREPROCESSING TRADER DATA ===\")\n",
    "trader_processed = preprocessor.preprocess_trader_data(trader_data.copy())\n",
    "\n",
    "print(\"New columns created:\")\n",
    "new_cols = set(trader_processed.columns) - set(trader_data.columns)\n",
    "print(list(new_cols))\n",
    "\n",
    "print(\"\\nProcessed data info:\")\n",
    "print(trader_processed.info())\n",
    "\n",
    "# Cell 3: Preprocess Sentiment Data\n",
    "print(\"=== PREPROCESSING SENTIMENT DATA ===\")\n",
    "sentiment_processed = preprocessor.preprocess_sentiment_data(sentiment_data.copy())\n",
    "\n",
    "print(\"New columns created:\")\n",
    "new_cols = set(sentiment_processed.columns) - set(sentiment_data.columns)\n",
    "print(list(new_cols))\n",
    "\n",
    "print(\"\\nSentiment score mapping:\")\n",
    "print(sentiment_processed[['Classification', 'sentiment_score']].drop_duplicates())\n",
    "\n",
    "# Cell 4: Handle Missing Values and Outliers\n",
    "print(\"=== HANDLING MISSING VALUES AND OUTLIERS ===\")\n",
    "\n",
    "# Check for extreme outliers in PnL\n",
    "q1 = trader_processed['closedPnL'].quantile(0.25)\n",
    "q3 = trader_processed['closedPnL'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 3 * iqr\n",
    "upper_bound = q3 + 3 * iqr\n",
    "\n",
    "outliers = trader_processed[(trader_processed['closedPnL'] < lower_bound) | \n",
    "                          (trader_processed['closedPnL'] > upper_bound)]\n",
    "print(f\"Extreme PnL outliers found: {len(outliers)}\")\n",
    "print(f\"Outlier range: {outliers['closedPnL'].min()} to {outliers['closedPnL'].max()}\")\n",
    "\n",
    "# Option to remove extreme outliers (uncomment if needed)\n",
    "# trader_processed = trader_processed[(trader_processed['closedPnL'] >= lower_bound) & \n",
    "#                                   (trader_processed['closedPnL'] <= upper_bound)]\n",
    "\n",
    "# Cell 5: Feature Engineering\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Create additional trading features\n",
    "if 'time' in trader_processed.columns:\n",
    "    trader_processed['time'] = pd.to_datetime(trader_processed['time'])\n",
    "    trader_processed['hour'] = trader_processed['time'].dt.hour\n",
    "    trader_processed['day_of_week'] = trader_processed['time'].dt.dayofweek\n",
    "    trader_processed['month'] = trader_processed['time'].dt.month\n",
    "\n",
    "# Risk metrics\n",
    "if 'size' in trader_processed.columns and 'leverage' in trader_processed.columns:\n",
    "    trader_processed['risk_exposure'] = trader_processed['size'] * trader_processed['leverage']\n",
    "\n",
    "# Profit margin\n",
    "if 'closedPnL' in trader_processed.columns and 'size' in trader_processed.columns:\n",
    "    trader_processed['profit_margin'] = trader_processed['closedPnL'] / trader_processed['size']\n",
    "    trader_processed['profit_margin'] = trader_processed['profit_margin'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(\"New features created:\")\n",
    "feature_cols = ['hour', 'day_of_week', 'month', 'risk_exposure', 'profit_margin']\n",
    "existing_features = [col for col in feature_cols if col in trader_processed.columns]\n",
    "print(existing_features)\n",
    "\n",
    "# Cell 6: Data Validation\n",
    "print(\"=== DATA VALIDATION ===\")\n",
    "\n",
    "# Check data consistency\n",
    "print(\"Data consistency checks:\")\n",
    "print(f\"Date range: {trader_processed['date'].min()} to {trader_processed['date'].max()}\")\n",
    "print(f\"PnL range: {trader_processed['closedPnL'].min()} to {trader_processed['closedPnL'].max()}\")\n",
    "print(f\"Size range: {trader_processed['size'].min()} to {trader_processed['size'].max()}\")\n",
    "\n",
    "# Check for data quality issues\n",
    "print(f\"\\nInfinite values: {np.isinf(trader_processed.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "print(f\"Null values: {trader_processed.isnull().sum().sum()}\")\n",
    "\n",
    "# Cell 7: Merge Datasets\n",
    "print(\"=== MERGING DATASETS ===\")\n",
    "merged_data = preprocessor.merge_datasets(trader_processed, sentiment_processed)\n",
    "\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Successful merge rate: {(merged_data['Classification'].notna().sum() / len(merged_data) * 100):.1f}%\")\n",
    "\n",
    "# Check merge results\n",
    "print(\"\\nMerge summary:\")\n",
    "print(merged_data['Classification'].value_counts())\n",
    "\n",
    "# Cell 8: Final Data Quality Check\n",
    "print(\"=== FINAL DATA QUALITY CHECK ===\")\n",
    "print(\"Final dataset info:\")\n",
    "print(merged_data.info())\n",
    "\n",
    "print(\"\\nFinal missing values:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "print(\"\\nFinal data shape:\", merged_data.shape)\n",
    "\n",
    "# Cell 9: Visualize Preprocessing Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Before/after PnL distribution\n",
    "axes[0,0].hist(trader_data['closedPnL'].dropna(), bins=50, alpha=0.7, label='Original')\n",
    "axes[0,0].hist(merged_data['closedPnL'].dropna(), bins=50, alpha=0.7, label='Processed')\n",
    "axes[0,0].set_title('PnL Distribution: Before vs After')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Sentiment distribution in merged data\n",
    "merged_data['Classification'].value_counts().plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Sentiment Distribution in Merged Data')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Daily trade counts\n",
    "if 'date' in merged_data.columns:\n",
    "    daily_counts = merged_data.groupby('date').size()\n",
    "    axes[1,0].plot(daily_counts.index, daily_counts.values)\n",
    "    axes[1,0].set_title('Daily Trade Counts')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Missing data heatmap\n",
    "missing_data = merged_data.isnull()\n",
    "sns.heatmap(missing_data.iloc[:, :10], ax=axes[1,1], cbar=True)\n",
    "axes[1,1].set_title('Missing Data Pattern (First 10 columns)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 10: Save Processed Data\n",
    "merged_data.to_csv('../data/processed/merged_data.csv', index=False)\n",
    "trader_processed.to_csv('../data/processed/trader_data_processed.csv', index=False)\n",
    "sentiment_processed.to_csv('../data/processed/sentiment_data_processed.csv', index=False)\n",
    "\n",
    "print(\" Preprocessing complete! All processed data saved.\")\n",
    "print(f\"Final merged dataset: {merged_data.shape[0]} rows, {merged_data.shape[1]} columns\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
